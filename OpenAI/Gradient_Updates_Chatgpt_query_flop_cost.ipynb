{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ_tDQuEqBWO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import minimize\n",
        "from sympy import divisors\n",
        "from dataclasses import dataclass\n",
        "import math\n",
        "from typing import Callable\n",
        "from copy import deepcopy\n",
        "import matplotlib.cm as cm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Model:\n",
        "    def __init__(self, name=\"None\", d_model=3*2**12, d_ff=9*2**12, ff_matrix_count=(1, 1), layers=120, n_experts=1, n_active_experts=1, num_query_heads=128, group_size=1, \\\n",
        "                 weight_precision_bytes=2, activation_precision_bytes=2, d_head=None, vocab_size=0, parallel_attention=False):\n",
        "        assert num_query_heads % group_size == 0\n",
        "\n",
        "        # Variables directly set\n",
        "        self.d_model = d_model\n",
        "        self.d_ff = d_ff\n",
        "        self.layers = layers\n",
        "        self.n_experts = n_experts\n",
        "        self.n_active_experts = n_active_experts\n",
        "        self.num_query_heads = num_query_heads\n",
        "        self.group_size = group_size\n",
        "        self.weight_precision_bytes = weight_precision_bytes\n",
        "        self.activation_precision_bytes = activation_precision_bytes\n",
        "        self.vocab_size = vocab_size\n",
        "        self.ff_matrix_count = ff_matrix_count\n",
        "        self.parallel_attention = parallel_attention\n",
        "\n",
        "        # Derived variables\n",
        "        self.ff_params_per_layer_per_expert = sum(self.ff_matrix_count) * self.d_model * self.d_ff\n",
        "        self.sparsity_factor = self.n_experts // self.n_active_experts\n",
        "        self.total_ff_params = self.layers * self.n_experts * self.ff_params_per_layer_per_expert\n",
        "        self.num_kv_heads = 2 * self.num_query_heads / self.group_size\n",
        "        self.d_head = d_head if d_head != None else self.d_model // self.num_query_heads\n",
        "        self.d_all_attn_heads = (self.num_query_heads + self.num_kv_heads) * self.d_head\n",
        "        self.attn_params_per_layer = self.d_all_attn_heads * self.d_model + self.d_head*self.num_query_heads*self.d_model\n",
        "\n",
        "        self.embedding_params = self.vocab_size * self.d_model * 2\n",
        "        self.total_attn_params = self.layers * self.attn_params_per_layer\n",
        "        self.total_params = self.total_attn_params + self.total_ff_params + self.embedding_params\n",
        "        self.total_active_params = self.total_attn_params + self.total_ff_params//self.sparsity_factor + self.embedding_params\n",
        "\n",
        "        self.kv_cache_size_per_input_bytes = self.num_kv_heads*self.d_head*self.layers*self.activation_precision_bytes\n",
        "\n",
        "        self.name = name\n",
        "\n",
        "    def __repr__(self):\n",
        "        representation = f\"\"\"Model Details:\n",
        "        Name: {self.name}\n",
        "        d_model: {self.d_model}\n",
        "        d_ff: {self.d_ff}\n",
        "        Depth: {self.layers}\n",
        "        Total FF Params: {self.total_ff_params}\n",
        "        Total Embedding Params: {self.embedding_params}\n",
        "        Num Attention Heads: {self.num_query_heads}\n",
        "        d_head: {self.d_head}\n",
        "        Group size: {self.group_size}\n",
        "        Total Attention Params: {self.total_attn_params}\n",
        "        Total Params: {self.total_params}\n",
        "        Total Active Params: {self.total_active_params}\n",
        "        \"\"\"\n",
        "        return representation\n",
        "\n",
        "    def arithmetic_cost_flop(self, input_len, batch_size, seq_len=1, count_masked_flop=False):\n",
        "        if count_masked_flop:\n",
        "          mean_input_len = input_len + seq_len\n",
        "        else:\n",
        "          mean_input_len = (input_len + (input_len + seq_len - 1))/2\n",
        "\n",
        "        # find cost to process prefill or decoding\n",
        "        # this scales quadratically with seq_len because mean_input_len is proportion to seq_len\n",
        "        return (2*self.total_active_params*batch_size*seq_len + 4*self.d_head*self.num_query_heads*self.layers*mean_input_len*batch_size*seq_len)\n",
        "\n",
        "\n",
        "# Mixtral 8x22B is an open-weight model with known architecture\n",
        "Mixtral_8x22B = Model(name=\"Mixtral 8x22B\",\n",
        "                      d_model=6144,\n",
        "                      d_ff=16384,\n",
        "                      ff_matrix_count=(2, 1),\n",
        "                      layers=56,\n",
        "                      n_experts=8,\n",
        "                      n_active_experts=2,\n",
        "                      num_query_heads=48,\n",
        "                      d_head=128,\n",
        "                      group_size=6,\n",
        "                      activation_precision_bytes=2,\n",
        "                      weight_precision_bytes=2,\n",
        "                      vocab_size=32000\n",
        ")\n",
        "\n",
        "def scale_model(name, model: Model, scale_factor: float, depth_exponent=1/3):\n",
        "    d_model = model.d_model * scale_factor**((1 - depth_exponent)/2)\n",
        "    d_ff = model.d_ff * scale_factor**((1 - depth_exponent)/2)\n",
        "    layers = int(model.layers * scale_factor**(depth_exponent))\n",
        "\n",
        "    num_query_heads = np.ceil(model.num_query_heads * scale_factor**((1 - depth_exponent)/4))\n",
        "    num_groups = model.num_query_heads/model.group_size\n",
        "    group_size = num_query_heads/num_groups\n",
        "\n",
        "    return Model(name=name,\n",
        "                 d_model=d_model,\n",
        "                 d_ff=d_ff,\n",
        "                 ff_matrix_count=model.ff_matrix_count,\n",
        "                 layers=layers,\n",
        "                 n_experts=model.n_experts,\n",
        "                 n_active_experts=model.n_active_experts,\n",
        "                 num_query_heads=num_query_heads,\n",
        "                 group_size=group_size,\n",
        "                 d_head=model.d_head * scale_factor**((1 - depth_exponent)/4),\n",
        "                 weight_precision_bytes=model.weight_precision_bytes,\n",
        "                 activation_precision_bytes=model.activation_precision_bytes,\n",
        "                 vocab_size=model.vocab_size,\n",
        "                 parallel_attention=model.parallel_attention\n",
        "                 )\n",
        "\n",
        "# This produces an estimate of GPT-4o's parameters by scaling up Mixtral 8x22B so that it has 100B active parameters\n",
        "GPT_4o = scale_model(\"GPT-4o\", Mixtral_8x22B, 2.6)"
      ],
      "metadata": {
        "id": "A6nGyPK5qHoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(GPT_4o)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JURToLYkFwWq",
        "outputId": "fd94f532-0b87-435a-9b9b-7067564d7b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Details:\n",
            "        Name: GPT-4o\n",
            "        d_model: 8448.423119303523\n",
            "        d_ff: 22529.128318142728\n",
            "        Depth: 77\n",
            "        Total FF Params: 351740204583.31104\n",
            "        Total Embedding Params: 540699079.6354254\n",
            "        Num Attention Heads: 57.0\n",
            "        d_head: 150.09706298972918\n",
            "        Group size: 7.125\n",
            "        Total Attention Params: 12693515805.99087\n",
            "        Total Params: 364974419468.9373\n",
            "        Total Active Params: 101169266030.6263\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_cost(input_len, output_len):\n",
        "    model_obj = GPT_4o\n",
        "\n",
        "    prefill_cost_flop = model_obj.arithmetic_cost_flop(input_len=0, batch_size=1, seq_len=input_len, count_masked_flop=True)\n",
        "    decoding_cost_flop = model_obj.arithmetic_cost_flop(input_len=input_len, batch_size=1, seq_len=output_len)\n",
        "\n",
        "    # assume 50% compute utilization during prefill. Kamath et. al. observed 70%: https://arxiv.org/pdf/2410.18038v1\n",
        "    prefill_utilization = 0.5\n",
        "    decoding_utilization = 0.1\n",
        "\n",
        "    # H100 server consuming up to 1500 W per GPU.\n",
        "    # Patel found ~100% TDP during prefill: https://www.microsoft.com/en-us/research/uploads/prod/2024/03/GPU_Power_ASPLOS_24.pdf\n",
        "    gpu_power_draw_watts = 1500\n",
        "    gpu_flop_per_second = 1e15\n",
        "\n",
        "    gpu_joules_per_flop = gpu_power_draw_watts/gpu_flop_per_second\n",
        "\n",
        "    print(\"FLOP and Wh cost for input length %d and output length %d:\" % (input_len, output_len))\n",
        "\n",
        "    print(\"Prefill cost: %.2e FLOP, %.3f Wh\" % (prefill_cost_flop, (1/prefill_utilization) * prefill_cost_flop*gpu_joules_per_flop/3600))\n",
        "    print(\"Decoding cost: %.2e FLOP, %.3f Wh\" % (decoding_cost_flop, (1/decoding_utilization) * decoding_cost_flop*gpu_joules_per_flop/3600))\n",
        "\n",
        "\n",
        "calculate_cost(10000, 500)\n",
        "calculate_cost(100000, 500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN0Ezyr4qXAQ",
        "outputId": "1ac31a81-54bb-4d6a-bbdb-3d798d572058"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLOP and Wh cost for input length 10000 and output length 500:\n",
            "Prefill cost: 2.29e+15 FLOP, 1.906 Wh\n",
            "Decoding cost: 1.15e+14 FLOP, 0.478 Wh\n",
            "FLOP and Wh cost for input length 100000 and output length 500:\n",
            "Prefill cost: 4.66e+16 FLOP, 38.821 Wh\n",
            "Decoding cost: 2.33e+14 FLOP, 0.972 Wh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2At5l9-IMpZZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}